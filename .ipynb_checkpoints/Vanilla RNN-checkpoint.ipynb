{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Goal: Predict how many 1 in each row of data\n",
    "https://peterroelants.github.io/posts/rnn-implementation-part01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ np.array([1,0,0]), np.array([1,1,0]), np.array([1,1,1])])\n",
    "Y = np.array( [1,2,3] )\n",
    "print X\n",
    "print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define forward step functions\n",
    "def update_state(xk, sk, wx, wRec):\n",
    "    \"\"\"\n",
    "    Compute state k from the previous state (sk) and current input (xk),\n",
    "    by use of the input weights (wk) and recursive weights (wRec).\n",
    "    \"\"\"\n",
    "    return xk*wx + sk*wRec\n",
    "\n",
    "def forward_states(X, wx, wRec):\n",
    "    \"\"\"\n",
    "    Unfold the network and compute all state activations givent the input X, input weights (wx),\n",
    "    and recursive weights (wRec). Return the state activations in a matrix, the last column S[:, -1]\n",
    "    contains the final activations.\n",
    "    \"\"\"\n",
    "    # initialize the matrix that holds all states for all input sequences\n",
    "    # The initial state s0 is set to 0\n",
    "    S = np.zeros((X.shape[0], X.shape[1] + 1))\n",
    "    \n",
    "    # Use the recurrence relation defined by update_state to update the states through time.\n",
    "    for k in range(0, X.shape[1]):\n",
    "        # S[k] = S[k-1]*wRec + X[k]*wx\n",
    "        S[:, k+1] = update_state(X[:,k], S[:,k], wx, wRec)\n",
    "    return S\n",
    "\n",
    "def loss(y,t):\n",
    "    \"\"\" MSE between the targets t and the outputs y.\"\"\"\n",
    "    return np.mean( (t - y)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_gradient(y, t):\n",
    "    \"\"\"\n",
    "    Gradient of the MSE loss function with respect to the output y.\n",
    "    \"\"\"\n",
    "    return 2.*(y - t)\n",
    "\n",
    "def backward_gradient(X, S, gradient_output, wRec):\n",
    "    \"\"\"\n",
    "    Backpropagate the gradient computed at the output (gradient_output) through the network. Accumuate the parameter\n",
    "    gradients for wX and wRec by for each layer by addition. Return the parameter gradients as a tuple, \n",
    "    and the gradients at the output of each layer.\n",
    "    \"\"\"\n",
    "    # Initialize the array that stores the gradients of the loss with respect to the states. \n",
    "    gradient_over_time = np.zeros((X.shape[0], X.shape[1] + 1))\n",
    "    gradient_over_time[:, -1] = gradient_output\n",
    "    \n",
    "    # Set the gradient accumulations to 0\n",
    "    wx_gradient = 0\n",
    "    wRec_gradient = 0\n",
    "    for k in range(X.shape[1], 0, -1):\n",
    "        # Compute the parameter gradients and accumulate the reuslts.\n",
    "        wx_gradient += np.sum(np.mean(gradient_over_time[:,k] * X[:, k-1], axis = 0))\n",
    "        wRec_gradient += np.sum(np.mean(gradient_over_time[:,k] * S[:, k-1]), axis =0)\n",
    "        # Compute the gradient at the output of the previous layer\n",
    "        gradient_over_time[:, k-1] = gradient_over_time[:,k]*wRec\n",
    "    return (wx_gradient, wRec_gradient), gradient_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7ac558c145cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Compute the backprop gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgradient_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mbackprop_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_over_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform Gradient Checking => the purpose to assert that we don't make any mistakes while computing the gradients\n",
    "\n",
    "# Set the weight parameters used during gradient checking\n",
    "params = [1.2, 1.2] # [wx, wRec]\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 1e-7\n",
    "\n",
    "# Compute the backprop gradients\n",
    "S = forward_states(X, params[0], params[1])\n",
    "gradient_output = output_gradient(S[:, -1], t)\n",
    "backprop_gradient, gradient_over_time = backward_gradient(X , S, gradient_output, params[1])\n",
    "\n",
    "# Compute the numerical gradient for each parameter in the layer\n",
    "for p_idx, _ in enumerate(params):\n",
    "    gradient_backprop = backprop_gradient[p_idx]\n",
    "    # +eps\n",
    "    params[p_idx] += eps\n",
    "    plus_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # -eps\n",
    "    minus_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # reset param value\n",
    "    params[p_idx] += eps\n",
    "    # calculate numerical gradient\n",
    "    gradient_numerical (plus_loss - minus_loss) / 2*eps\n",
    "    # Raise error if the numerical grade is not close to the backprop gradient\n",
    "    if not np.isclose(gradient_numerical, gradient_backprop):\n",
    "        print 'there is a gradient error' \n",
    "#         ValueError((\n",
    "#             f'Numerical gradient of {gradient_numerical:.6f} is not close to '\n",
    "#             f'the backpropagation gradient of {gradient_backprop: .6f}'))\n",
    "print 'No gradient errors found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
