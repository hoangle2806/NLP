{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0373, -0.0735,  1.0402],\n",
       "         [-1.7711,  0.2087, -0.6117],\n",
       "         [ 0.2580, -0.7050, -1.4120]],\n",
       "\n",
       "        [[ 1.2683,  0.4034, -0.6735],\n",
       "         [-0.6236, -0.1220,  0.7871],\n",
       "         [-0.1204,  1.1041, -0.4455]],\n",
       "\n",
       "        [[-0.8673, -0.3652, -0.7961],\n",
       "         [-1.3853,  0.4172,  0.2685],\n",
       "         [-1.3529,  0.2131, -0.0333]],\n",
       "\n",
       "        [[ 0.3101,  2.0596,  0.0679],\n",
       "         [ 1.1043,  1.0056, -0.1573],\n",
       "         [ 0.2065,  0.2897, -1.5386]],\n",
       "\n",
       "        [[ 1.8807, -0.3382, -0.1565],\n",
       "         [ 1.0637, -0.0542, -0.4151],\n",
       "         [ 2.3975, -0.8450,  1.7894]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6559, 0.3769, 0.1083]],\n",
      "\n",
      "        [[0.3344, 0.4941, 0.2223]],\n",
      "\n",
      "        [[0.3133, 0.4460, 0.9742]],\n",
      "\n",
      "        [[0.8162, 0.9805, 0.1006]],\n",
      "\n",
      "        [[0.1802, 0.3852, 0.5362]]])\n",
      "==============================\n",
      "tensor([[[0.3231]],\n",
      "\n",
      "        [[0.7324]],\n",
      "\n",
      "        [[0.4898]],\n",
      "\n",
      "        [[0.5008]],\n",
      "\n",
      "        [[0.7140]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 1, 3) # (seq_len, batch, input_size)\n",
    "y = torch.rand(5, 1, 1) \n",
    "print x \n",
    "print '=============================='\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x and y represents sequence morels where it takes 2 vectors , 2 feature in each vector to predict 1 vector, 2 features in a vector. It represent 2 word vectors to predict 1 word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0543,  0.5341, -0.2104, -0.3447,  0.3795,  0.5987, -0.3770]],\n",
      "\n",
      "        [[-0.3158,  0.5657, -0.1412, -0.4489,  0.2254,  0.7343,  0.2773]],\n",
      "\n",
      "        [[-0.5827,  0.6934, -0.0971, -0.6014,  0.4986,  0.7195,  0.3857]],\n",
      "\n",
      "        [[-0.6377,  0.7622, -0.1127, -0.4886,  0.1535,  0.7968,  0.4705]],\n",
      "\n",
      "        [[-0.6905,  0.7419, -0.0039, -0.5464,  0.3482,  0.7428,  0.4258]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "===============================\n",
      "tensor([[[-0.4494, -0.4511,  0.5816,  0.8025,  0.2321,  0.7323, -0.5229]],\n",
      "\n",
      "        [[-0.6905,  0.7419, -0.0039, -0.5464,  0.3482,  0.7428,  0.4258]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size= 3,hidden_size= 7, num_layers= 2) # 3 features for each input, and 2 RNN units (or number of neurons) names depend on documentation\n",
    "hidden = None\n",
    "output, hn = rnn(x,hidden) # output = output according to inputs\n",
    "                    # hn = hidden state\n",
    "# Linear it out\n",
    "print output\n",
    "print '==============================='\n",
    "print hn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN matrix\n",
      "Parameter containing:\n",
      "tensor([[-0.3678, -0.0015,  0.2291],\n",
      "        [-0.0106,  0.3414,  0.2649],\n",
      "        [-0.1292,  0.0700,  0.2053],\n",
      "        [-0.2131,  0.2646,  0.2838],\n",
      "        [-0.1593,  0.2921,  0.2913],\n",
      "        [ 0.2753,  0.1721,  0.3254],\n",
      "        [ 0.3736,  0.0064, -0.3253]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0168,  0.3074, -0.0246,  0.2405,  0.3700, -0.2589, -0.0806],\n",
      "        [ 0.2663, -0.1581, -0.3186, -0.3394, -0.2042, -0.3239, -0.1070],\n",
      "        [-0.0426,  0.1421,  0.3690, -0.1035, -0.3543,  0.0059, -0.0240],\n",
      "        [-0.0831, -0.1801,  0.2835,  0.0690,  0.2869,  0.3515,  0.1730],\n",
      "        [ 0.3568, -0.2133, -0.3221,  0.2182, -0.3526,  0.0610,  0.2913],\n",
      "        [ 0.0600,  0.3494,  0.1750, -0.1133, -0.2978,  0.1451, -0.3331],\n",
      "        [-0.1683,  0.3518, -0.3243, -0.1255, -0.3317,  0.2252, -0.2090]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3624,  0.2104,  0.0812,  0.0486, -0.2397,  0.2629,  0.2051],\n",
      "        [-0.1124,  0.0517,  0.2292,  0.2086, -0.3307, -0.1308, -0.3747],\n",
      "        [-0.1082,  0.0985, -0.2654,  0.1294, -0.1395, -0.3370, -0.2470],\n",
      "        [ 0.0031,  0.0435, -0.2900,  0.0176, -0.0097, -0.1928,  0.3461],\n",
      "        [ 0.1392,  0.0382,  0.1513, -0.0832,  0.2837,  0.3622, -0.1332],\n",
      "        [-0.1900, -0.1511,  0.1506,  0.1724, -0.2868,  0.3347,  0.1265],\n",
      "        [ 0.2911, -0.3061,  0.3498,  0.0542, -0.1526,  0.1115, -0.1770]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0889, -0.3468,  0.3021,  0.1460, -0.1982, -0.2028, -0.2874],\n",
      "        [-0.2448,  0.1399,  0.1669, -0.1344,  0.1457, -0.2701,  0.1646],\n",
      "        [ 0.0468,  0.3085,  0.3036,  0.1696, -0.1969,  0.2108, -0.0373],\n",
      "        [ 0.0095,  0.1557,  0.3553, -0.1578, -0.1763, -0.1566, -0.1541],\n",
      "        [ 0.3516, -0.3732, -0.3354, -0.3023, -0.3367,  0.1754,  0.2773],\n",
      "        [-0.1174, -0.1680, -0.0758, -0.1179, -0.0596,  0.1799,  0.0396],\n",
      "        [ 0.1126,  0.3246,  0.0867, -0.1603,  0.3438,  0.2005,  0.2082]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print \"RNN matrix\"\n",
    "print rnn.weight_ih_l0\n",
    "print rnn.weight_hh_l0 # print out first hidden matrix\n",
    "print rnn.weight_ih_l1\n",
    "print rnn.weight_hh_l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4661],\n",
      "        [0.6541],\n",
      "        [0.8547],\n",
      "        [0.7880],\n",
      "        [0.8086]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Linear layer to convert to 1 output\n",
    "linear = nn.Linear(in_features= 7, out_features= 1)\n",
    "outputs = linear(output.contiguous().view(-1, 7))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8592, 0.5847, 0.1068]],\n",
      "\n",
      "        [[0.6583, 0.3575, 0.6971]],\n",
      "\n",
      "        [[0.9692, 0.8188, 0.5138]]])\n",
      "==================================\n",
      "tensor([[[-1.5880, -0.4621]]])\n",
      "==================================\n",
      "tensor([[[0.5169, 0.6440]]])\n",
      "===============lstm_output===================\n",
      "tensor([[[-0.0773,  0.4154]],\n",
      "\n",
      "        [[-0.1303,  0.2067]],\n",
      "\n",
      "        [[-0.1377,  0.2027]],\n",
      "\n",
      "        [[-0.2234,  0.1024]],\n",
      "\n",
      "        [[-0.1888,  0.1681]]], grad_fn=<StackBackward>)\n",
      "======================\n",
      "tensor([[[-0.1888,  0.1681]]], grad_fn=<StackBackward>)\n",
      "======================\n",
      "tensor([[[-0.9719,  0.3206]]], grad_fn=<StackBackward>)\n",
      "=========== Weights ===========\n",
      "Parameter containing:\n",
      "tensor([[ 0.4274,  0.3160,  0.3473],\n",
      "        [ 0.1594,  0.0123, -0.5514],\n",
      "        [-0.5300,  0.2162, -0.0701],\n",
      "        [ 0.1214,  0.4334,  0.2359],\n",
      "        [-0.0678,  0.4457, -0.0788],\n",
      "        [-0.3845, -0.6051,  0.4555],\n",
      "        [-0.2588,  0.5409, -0.5200],\n",
      "        [ 0.3884, -0.6742, -0.2667]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2729, -0.5497],\n",
      "        [ 0.4751,  0.3594],\n",
      "        [ 0.4455,  0.2415],\n",
      "        [-0.2792, -0.6501],\n",
      "        [-0.1177, -0.1457],\n",
      "        [-0.1307, -0.1419],\n",
      "        [ 0.1737, -0.7019],\n",
      "        [-0.5906, -0.2908]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# prep data\n",
    "x_lstm = torch.rand(3, 1, 3)\n",
    "h0 = torch.randn(1, 1, 2) # initial hidden state for each element in batch\n",
    "c0 = torch.randn(1, 1, 2)  # intial cell state for each element in batch\n",
    "# LSTM cell\n",
    "lstm = nn.LSTM(input_size= 3,hidden_size= 2, num_layers = 1)\n",
    "lstm_hidden = None\n",
    "\n",
    "lstm_output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "print x_lstm\n",
    "print '=================================='\n",
    "print h0\n",
    "print '=================================='\n",
    "print c0\n",
    "print '===============lstm_output==================='\n",
    "print lstm_output\n",
    "print '======================'\n",
    "print hn\n",
    "print '======================'\n",
    "print cn\n",
    "print '=========== Weights ==========='\n",
    "print lstm.weight_ih_l0\n",
    "print lstm.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.996695  , 0.82524577],\n",
       "       [0.5361272 , 0.23388793],\n",
       "       [0.36633005, 0.33796579],\n",
       "       [0.53265547, 0.31574424],\n",
       "       [0.50184816, 0.64860111]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep data\n",
    "X = np.random.rand(3,3)\n",
    "Y = np.random.rand(3,1)\n",
    "h0 = np.random.rand(1,2)\n",
    "c0 = np.random.rand(1,2)\n",
    "\n",
    "W_f = np.random.rand(X.shape[1] + c0.shape[1],c0.shape[1])\n",
    "W_i = np.random.rand(X.shape[1] + c0.shape[1],c0.shape[1])\n",
    "W_c = np.random.rand(X.shape[1] + c0.shape[1],c0.shape[1])\n",
    "W_o = np.random.rand(X.shape[1] + c0.shape[1],c0.shape[1])\n",
    "W_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80393731, 0.75010412])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forget gate\n",
    "concat_X0_h0 = np.concatenate((X[0],h0), axis = None)\n",
    "forget_vector = np.dot(concat_X0_h0,W_f) # Dot already did the transpose for us\n",
    "numpy_sigmoid = np.vectorize(sigmoid)\n",
    "f_t = numpy_sigmoid(forget_vector) # f_t labeled according to link on the top\n",
    "f_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input gate\n",
    "i_t = numpy_sigmoid(np.dot(concat_X0_h0,W_i))\n",
    "tanh_vectorize = np.vectorize(np.tanh)\n",
    "before_C_t = tanh_vectorize(np.dot(concat_X0_h0, W_c)) # denoted C~_t in the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0766217 , 0.88996784]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update cell state\n",
    "C_t = np.multiply(f_t,c0) + np.multiply(i_t, before_C_t)\n",
    "C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output gate\n",
    "o_t = numpy_sigmoid(np.dot(concat_X0_h0, W_o))\n",
    "h_t = np.multiply(o_t, np.tanh(C_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of h_t for x_t: [[0.59175221 0.50740964]]\n"
     ]
    }
   ],
   "source": [
    "print \"Prediction of h_t for x_t: \" + str(h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get to prediction, feed this through another Dense Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
