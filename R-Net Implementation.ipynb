{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This link will explain concepts of the paper:\n",
    "https://codeburst.io/understanding-r-net-microsofts-superhuman-reading-ai-23ff7ededd96\n",
    "\n",
    "This link provided implementations in tensorflow.\n",
    "https://github.com/NLPLearn/R-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    \n",
    "    # data\n",
    "    data_size = -1\n",
    "    num_epochs = 10\n",
    "    train_prop = 0.9\n",
    "    \n",
    "    # Training\n",
    "        # NOTE: To use demo, put batch_size == 1\n",
    "    mode = \"train\" # case-insensitive options: ['train', 'test', 'debug']\n",
    "    dropout = 0.2\n",
    "    zoneout = None \n",
    "    optimizer = \"adam\" # Options: ['adadelta', 'adam', 'gradientdescent', 'adagrad']\n",
    "    batch_size = 50 if mode is not \"test\" else 100 #size of the mini-batch for training\n",
    "    save_steps = 50\n",
    "    clip = True # clip gradient norm\n",
    "    norm = 5.0 \n",
    "    # NOTE: Change the hyperparameters of your learning algos here\n",
    "    opt_arg = {\n",
    "        'adadelta':{'learning_rate':1, 'rho': 0.95, 'epsilon':1e-6},\n",
    "        'adam':{'learning_rate':1e-3, 'beta1':0.9, 'beta2':0.999, 'epsilon':1e-8},\n",
    "        'gradientdescent':{'learning_rate':1},\n",
    "        'adagrad':{'learning_rate':1}\n",
    "    }\n",
    "    \n",
    "    # Architecture\n",
    "    SRU = True # Use SRU cell, if False, use standard GRU cell\n",
    "    max_p_len = 300 # Maximum number of words in each passage context\n",
    "    max_q_len = 30 # Maximum number of words in each question context\n",
    "    max_char_len = 16 # Maximum number of characters in a word\n",
    "    vocab_size = 91605 # number of vocabs after Glove training\n",
    "    char_vocab_size = 95 # number of charaters in Glove\n",
    "    emb_size = 300 # Embeddings size for words\n",
    "    char_emb_size = 8 # Embedding size for characters\n",
    "    attn_size = 75 # RNN cell and attention module size\n",
    "    num_layers = 3 # Number of layers at question-passage matching\n",
    "    bias = True # Use bias term in attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import spacy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import numbers\n",
    "\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn import RNNCell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Layers for gated attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attention weights from https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\n",
    "W_u^Q.shape:    (2 * attn_size, attn_size)\n",
    "W_u^P.shape:    (2 * attn_size, attn_size)\n",
    "W_v^P.shape:    (attn_size, attn_size)\n",
    "W_g.shape:      (4 * attn_size, 4 * attn_size)\n",
    "W_h^P.shape:    (2 * attn_size, attn_size)\n",
    "W_v^Phat.shape: (2 * attn_size, attn_size)\n",
    "W_h^a.shape:    (2 * attn_size, attn_size)\n",
    "W_v^Q.shape:    (attn_size, attn_size)\n",
    "'''\n",
    "\n",
    "def get_attn_params(attn_size, initializer = tf.truncated_normal_initializer):\n",
    "    '''\n",
    "    Args:\n",
    "        attn_size: the size of attention specified in https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\n",
    "        initializer: the author of the original paper used Gaussian initialization, however xavier converge faster\n",
    "    Returns:\n",
    "        params: A collection of parameters used throughout the layers\n",
    "    '''\n",
    "    with tf.variable_scope(\"attention_weights\"):\n",
    "        params = {\n",
    "            \"W_u_Q\": tf.get_variable(\"W_u_Q\", dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_u_P\": tf.get_variable(\"W_u_P\", dtype = tf.float32, shape = (2 * attn_size, 2*attn_size), initializer = initializer()),\n",
    "            \"W_v_P\": tf.get_variable(\"W_v_P\", dtype = tf.float32, shape = (attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_v_P_2\": tf.get_variable(\"W_v_P_2\", dtype = tf.float32, shape = (2*attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_g\": tf.get_variable(\"W_g\", dtype = tf.float32, shape = (4*attn_size, 4*attn_size), initializer = initializer()),\n",
    "            \"W_h_P\": tf.get_variable(\"W_h_P\", dtype = tf.float32, shape = (2*attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_v_Phat\": tf.get_variable(\"W_v_Phat\", dtype = tf.float32, shape = (2*attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_h_a\": tf.get_variable(\"W_h_a\", dtype = tf.float32, shape = (2*attn_size, attn_size), initializer = initializer()),\n",
    "            \"W_v_Q\": tf.get_variable(\"W_v_Q\", dtype = tf.float32, shape = (attn_size, attn_size), initializer = initializer()),\n",
    "            \"v\": tf.get_variable(\"v\", dtype = tf.float32, shape = (attn_size), initializer = initializer())\n",
    "        }\n",
    "        return params\n",
    "\n",
    "def encoding(word, char, word_embeddings, char_embeddings, scope = \"embedding\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        word_encoding = tf.nn.embedding_lookup(word_embeddings, word)\n",
    "        char_encoding = tf.nn.embedding_lookup(char_embeddings, char)\n",
    "        return word_encoding, char_encoding\n",
    "\n",
    "def apply_dropout(inputs, size = None, is_training = True):\n",
    "    '''\n",
    "    Implementation of ZoneOut from https://arxiv.org/pdf/1606.01305.pdf\n",
    "    '''\n",
    "    if Params.dropout is None and Params.zoneout is None:\n",
    "        return inputs\n",
    "    if Params.zoneout is not None:\n",
    "        return ZoneoutWrapper(inputs, state_zoneout_prob = Params.zoneout, is_training = is_training)\n",
    "    elif is_training:\n",
    "        return tf.contrib.rnn.DropoutWrapper(inputs,\n",
    "                                            output_keep_prob = 1 - Params.dropout,\n",
    "                                            dtype = tf.float32)\n",
    "    else:\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def bidirectional_GRU(inputs, inputs_len, cell = None, cell_fn = tf.contrib.rnn.GRUCell, units = Params.attn_size,\n",
    "                     layers = 1, scope = \"Bidirectional_GRU\", output = 0, is_training = True, reuse = None):\n",
    "    '''\n",
    "    Bidirectional RNN with GRU cells.\n",
    "    \n",
    "    Args:\n",
    "        inputs: rnn input of shape (batch_size, timestep, dim)\n",
    "        inputs_len: rnn input_len of shape (batch_size, )\n",
    "        cell: rnn cell of type RNN_Cell.\n",
    "        output: if 0, output returns rnn output for every timestep,\n",
    "                if 1, output returns concatenated state of backward and forward rnn.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse = reuse):\n",
    "        if cell is not None:\n",
    "            (cell_fw, cell_bw) = call\n",
    "        else:\n",
    "            shapes = inputs.get_shape().as_list()\n",
    "            if len(shapes) > 3:\n",
    "                inputs = tf.reshape(inputs, (shapes[0]*shapes[1], shapes[2], -1))\n",
    "                inputs_len = tf.reshape(inputs_len, (shapes[0]*shapes[1]))\n",
    "                \n",
    "            # if no cells are provided, use standard GRU cell implementation\n",
    "            if layers > 1 :\n",
    "                cell_fw = MultiRNNCell([apply_dropout(cell_fn(units), size = inputs.shape[-1] if i == 0 else units, \n",
    "                                                      is_training = is_training) for i in range(layers)])\n",
    "                cell_bw = MultiRNNCell([apply_dropout(cell_fn(units), size = inputs.shape[-1] if i == 0 else units,\n",
    "                                                     is_training = is_training) for i in range(layers)])\n",
    "            else:\n",
    "                cell_fw, cell_bw = [apply_dropout(cell_fn(units), size = inputs.shape[-1], is_training = is_training) for _ in range(2)]\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs,\n",
    "                                                         sequence_length = inputs_len,\n",
    "                                                         dtype = tf.float32)\n",
    "        \n",
    "        if output == 0:\n",
    "            return tf.concat(outputs, 2)\n",
    "        elif output == 1:\n",
    "            return tf.reshape(tf.concat(states, 1), (Params.batch_size, shapes[1], 2*units))\n",
    "        \n",
    "\n",
    "def pointer_net(passage, passage_len, question, question_len, cell, params, scope = \"pointer_network\"):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
