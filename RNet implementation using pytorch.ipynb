{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "char_limit = 16\n",
    "char_dim = 8\n",
    "char_hidden_size = 100\n",
    "char_num_layers = 1\n",
    "char_dir = 2\n",
    "\n",
    "dropout = 0.2\n",
    "batch_size = 24\n",
    "hidden_size = 75\n",
    "word_emb_size = 300 \n",
    "char_emb_size = char_dir*char_num_layers*char_hidden_size\n",
    "emb_size = word_emb_size + char_emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = utils.Logger()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bidirectional gru hidden state to represent char embedding for a word\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, in_size= word_emb_size):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = True\n",
    "        self.dir = 2 if self.bidirectional else 1\n",
    "        self.hidden_size = char_hidden_size\n",
    "        self.in_size = in_size\n",
    "        self.gru = nn.GRU(input_size= in_size, bidirectional= self.bidirectional, num_layers= self.num_layers)\n",
    "        self.h = torch.randn(self.num_layers*self.dir, 1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        (l, b, in_size) = input.size()\n",
    "        assert in_size == self.in_size and b == 1\n",
    "        o, h = self.gru(input, self.h)\n",
    "        h = h.view(-1) # the view function is to reshape the tensor, \n",
    "                        # -1 means you can extends this to tensors with more columns\n",
    "        return h\n",
    "    \n",
    "# Input is the concatenation of word embedding and its corresponding char embedding\n",
    "# Output is passage embedding or question embedding\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bidirectional = True\n",
    "        self.dir = 2 if self.bidirectional else 1\n",
    "        self.num_layers = 3\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_size = in_size\n",
    "        self.gru = nn.GRU(input_size= in_size, bidirectional= self.bidirectional, num_layers= self.num_layers,\n",
    "                         hidden_size= self.hidden_size)\n",
    "        self.out_size = self.hidden_size*self.num_layers*self.dir\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        (l, _, in_size) = input.size()\n",
    "        hs = torch.zeros(l, self.num_layers*self.dir, batch_size, hidden_size).to(device)\n",
    "        h = torch.randn(self.num_layers*self.dir, batch_size, self.hidden_size).to(device)\n",
    "        input = torch.unsqueeze(input, dim = 1)\n",
    "        for i in range(l):\n",
    "            self.gru.flatten_parameters()\n",
    "            _, h = self.gru(input[i], h)\n",
    "            hs[i] = h\n",
    "        del h, input\n",
    "        hs_ = hs.permute([0,2,1,3]).contiguous(),view(l, batch_size, -1)\n",
    "        hs = self.dropout(hs_)\n",
    "        return hs\n",
    "    \n",
    "\n",
    "# Using passage and question to obtain question-aware passage representation\n",
    "# Co-attention\n",
    "class PQMatcher(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(PQMatcher, self).__init__()\n",
    "        self.hidden_size = hidden_size*2\n",
    "        self.in_size = in_size\n",
    "        self.gru = nn.GRUCell(input_size=in_size*2, hidden_size=self.hidden_size)\n",
    "        self.Wp = nn.Linear(self.in_size*2, self.hidden_size, bias = False)\n",
    "        self.Wq = nn.Linear(self.in_size*2, self.hidden_size, bias = False)\n",
    "        self.Wv = nn.Linear(self.hidden_size, self.hidden_size, bias = False)\n",
    "        self.Wg = nn.Linear(self.in_size*4, self.in_size*4, bias = False)\n",
    "        self.out_size = self.hidden_size\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, up, uq):\n",
    "        (lp, _, _) = up.size()\n",
    "        (lq, _, _) = up.size()\n",
    "        mixerp, mixerq = torch.arange(lp).long().to(device), torch.arange(lq).long().to(device)\n",
    "        Up = torch.cat([up, up[mixerp]], dim = 2)\n",
    "        Uq = torch.cat([uq, uq[mixerq]], dim = 2)\n",
    "        vs = torch.zeros(lp, batch_size, self.out_size).to(device)\n",
    "        v = torch.randn(batch_size, self.hidden_size).to(device)\n",
    "        V = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        \n",
    "        Uq_ = Uq.permute([1,0,2])\n",
    "        for i in range(lp):\n",
    "            Wup = self.Wp(Up[i])\n",
    "            Wuq = self.Wq(Uq)\n",
    "            Wvv = self.Wv(v)\n",
    "            x = F.tanh(Wup, Wuq + Wvv).permute([1,0,2])\n",
    "            s = torch.bmm(x, V)\n",
    "            s = torch.squeeze(s, 2)\n",
    "            a = F.softmax(s, 1).unsqueeze(1)\n",
    "            c = torch.bmm(a, Uq_).squeeze()\n",
    "            r = torch.cat([Up[i], c], dim = 1)\n",
    "            g = F.sigmoid(self.Wg(r))\n",
    "            r_ = torch.mul(g, r)\n",
    "            c_ = r_[:, self.in_size*2:]\n",
    "            v = self.gru(c_, v)\n",
    "            vs[i] = v\n",
    "            del Wup, Wuq, Wvv, x, a, s, c, g, r, r_, c_\n",
    "        del up, uq, Up, Uq, Uq_\n",
    "        vs = self.dropout(vs)\n",
    "        return vs\n",
    "\n",
    "\n",
    "# Input is question-aware passage representation\n",
    "# Output is self-attention question-aware passage representation.\n",
    "class SelfMatcher(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(SelfMatcher, self).__init__()\n",
    "        self.hidden_size = in_size\n",
    "        self.in_size = in_size\n",
    "        self.gru = nn.GRUCell(input_size=in_size, hidden_size=self.hidden_size)\n",
    "        self.Wp = nn.Linear(self.in_size, self.hidden_size, bias=False)\n",
    "        self.Wp_ = nn.Linear(self.in_size, self.hidden_size, bias=False)\n",
    "        self.out_size = self.hidden_size\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, v):\n",
    "        (l, _, _) = v.size()\n",
    "        h = torch.randn(batch_size, self.hidden_size).to(device)\n",
    "        V = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        hs = torch.zeros(l, batch_size, self.out_size).to(device)\n",
    "        \n",
    "        for i in range(l):\n",
    "            Wpv = self.Wp(v[i])\n",
    "            Wpv_ = self.Wp_(v)\n",
    "            x = F.tanh(Wpv + Wpv_)\n",
    "            x = x.permute([1, 0, 2])\n",
    "            s = torch.bmm(x, V)\n",
    "            s = torch.squeeze(s, 2)\n",
    "            a = F.softmax(s, 1).unsqueeze(1)\n",
    "            c = torch.bmm(a, v.permute([1,0,2])).squeeze()\n",
    "            h = self.gru(c, h)\n",
    "            hs[i] = h\n",
    "            del h, v\n",
    "            return hs\n",
    "\n",
    "        \n",
    "# Input is question representation and self-attention question-aware passage representation\n",
    "# Output are start and end pointer distribution\n",
    "class Pointer(nn.Module):\n",
    "    def __init__(self, in_size1, in_size2):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.hidden_size = in_size2\n",
    "        self.in_size1 = in_size1\n",
    "        self.in_size2 = in_size2\n",
    "        self.gru = nn.GRUCell(input_size=in_size1, hidden_size=self.hidden_size)\n",
    "        # Wu uses bias. See formula (11) in the paper. Maybe Vr is just a bias\n",
    "        self.Wu = nn.Linear(self.in_size2, self.hidden_size, bias = True)\n",
    "        self.Wh = nn.Linear(self.in_size1, self.hidden_size, bias = False)\n",
    "        self.Wha = nn.Linear(self.in_size2, self.hidden_size, bias = False)\n",
    "        self.out_size = 1\n",
    "        \n",
    "    def forward(self, h, u):\n",
    "        (lp, _, _) = h.size()\n",
    "        (lq, _, _) = u.size()\n",
    "        v = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        u_ = u.permute([1,0,2])\n",
    "        h_ = h.permute([1,0,2])\n",
    "        x = F.tanh(self.Wu(u)).permute([1,0,2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s, 2)\n",
    "        a = F.softmax(s, 1).unsqueeze(1)\n",
    "        r = torch.bmm(a, u_).squeeze()\n",
    "        x = F.tanh(self.Wh(h) + self.Wha(r)).permute([1,0,2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s)\n",
    "        p1 = F.softmax(s, 1)\n",
    "        c = torch.bmm(p1.unsqueeze(1), h_).squeeze()\n",
    "        r = self.gru(c, r)\n",
    "        x = F.tanh(self.Wh(h) + self.Wha(r)).permute([1,0,2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s)\n",
    "        p2 = F.softmax(s, 1)\n",
    "        return (p1, p2)\n",
    "    \n",
    "\n",
    "class RNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.encoder = Encoder(emb_size)\n",
    "        self.pqmatcher = PQMatcher(self.encoder.out_size)\n",
    "        self.selfmatcher = SelfMatcher(self.pqmatcher.out_size)\n",
    "        self.pointer = Pointer(self.selfmatcher.out_size, self.encoder.out_size)\n",
    "        \n",
    "    # wemb of P, cemb of P, w of Q, c of Q, Answer\n",
    "    def forward(self, Pw, Pc, Qw, Qc):\n",
    "        lp = Pw.size()[0]\n",
    "        lq = Qw.size()[0]\n",
    "        P = torch.cat([Pw, Pc], dim = 2)\n",
    "        Q = torch.cat([Qw, Qc], dim = 2)\n",
    "        Up = self.encoder(P)\n",
    "        Uq = self.encoder(Q)\n",
    "        v = self.pqmatcher(Up, Uq)\n",
    "        torch.cuda.empty_cache()\n",
    "        h = self.selfmatcher(v)\n",
    "        p1, p2 = self.pointer(h, Uq)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
